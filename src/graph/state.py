"""LangGraph state definition for quiz generation workflow."""

from typing import Annotated, Any

from typing_extensions import TypedDict

from src.models.quiz import Question, Quiz, QuizRound, UserInput


# Merge functions ensure that the agents don't overwrite each others work or it's own previous work
def merge_questions(existing: list[Question], new: list[Question]) -> list[Question]:
    """
    Merge question lists by replacing questions with matching IDs.

    On regeneration, new questions replace old ones with the same ID.
    Questions without matches are added to the list.
    """
    if not existing:
        return new
    if not new:
        return existing

    # Create a dictionary of existing questions by ID for fast lookup
    existing_dict = {q.id: q for q in existing}

    # Replace or add questions from new list
    for question in new:
        existing_dict[question.id] = question

    # Return as list, preserving order as much as possible
    return list(existing_dict.values())


def merge_rounds(existing: list[QuizRound], new: list[QuizRound]) -> list[QuizRound]:
    """Merge round lists."""
    if not existing:
        return new
    if not new:
        return existing

    result = existing.copy()
    for new_round in new:
        for i, existing_round in enumerate(result):
            if existing_round.round_number == new_round.round_number:
                result[i] = new_round
                break
        else:
            result.append(new_round)
    return result


class QuizState(TypedDict, total=False):
    """
    State object shared across all agents in the quiz generation workflow.

    This state is passed through the LangGraph workflow and updated by each agent.
    Using Annotated with reducer functions for proper state merging.
    """

    # User input
    user_input: UserInput

    # Planning phase
    quiz_plan: dict[str, Any] | None
    """
    Quiz plan created by Planner Agent.
    Structure:
    {
        "title": str,
        "description": str,
        "rounds": [
            {
                "round_number": int,
                "round_name": str,
                "topic": str,
                "question_count": int,
                "difficulty": str
            },
            ...
        ]
    }
    """

    # Generation phase
    raw_questions: Annotated[list[Question], merge_questions]
    """Raw questions generated by Question Generator Agent(s)."""

    # Review phase
    reviewed_questions: Annotated[list[Question], merge_questions]
    """Questions after quality review with scores and feedback."""

    review_feedback: list[dict[str, Any]] | None
    """Detailed feedback from Quality Reviewer Agent."""

    # Validation phase
    validated_questions: Annotated[list[Question], merge_questions]
    """Questions validated by Answer Validator Agent."""

    validation_issues: list[dict[str, Any]] | None
    """Any issues found during validation."""

    # Final quiz
    quiz_rounds: Annotated[list[QuizRound], merge_rounds]
    """Organized quiz rounds."""

    final_quiz: Quiz | None
    """Complete quiz object ready for export."""

    # Control flow
    feedback_loop_count: int
    """Number of times questions have been regenerated."""

    max_regeneration_attempts: int
    """Maximum allowed regeneration attempts (default: 3)."""

    current_round_index: int | None
    """Track which round is being processed (for parallel generation)."""

    needs_regeneration: bool
    """Flag indicating if questions need to be regenerated."""

    # Metadata
    errors: Annotated[list[str], lambda x, y: x + y]
    """Accumulate any errors during the workflow."""

    quality_threshold: float
    """Minimum quality score to accept questions (default: 0.7)."""


def create_initial_state(user_input: UserInput) -> QuizState:
    """
    Create the initial state for the quiz generation workflow.

    Args:
        user_input: User's quiz requirements

    Returns:
        Initial QuizState with default values
    """
    return QuizState(
        user_input=user_input,
        quiz_plan=None,
        raw_questions=[],
        reviewed_questions=[],
        review_feedback=None,
        validated_questions=[],
        validation_issues=None,
        quiz_rounds=[],
        final_quiz=None,
        feedback_loop_count=0,
        max_regeneration_attempts=3,
        current_round_index=None,
        needs_regeneration=False,
        errors=[],
        quality_threshold=0.7,
    )
